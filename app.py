import os
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI

# .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
load_dotenv()

client = OpenAI(
    api_key=os.getenv("GROQ_API_KEY"),
    base_url="https://api.groq.com/openai/v1"
)

# Streamlit ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="ğŸ§  ê¹€í˜„ìˆ˜ ì±—ë´‡ ê°–ë‹¤ë¶™ì´ê¸° ì—°ìŠµìš©", page_icon="ğŸ’¬")
st.title("ğŸ§  Gemma2-9B-it ëª¨ë¸ API ì±—ë´‡")
st.markdown("""
# ğŸ‘‹ í™˜ì˜í•©ë‹ˆë‹¤!
ì´ê³³ì€ ì±—ë´‡ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.  
ì•„ë˜ ì…ë ¥ì°½ì— ì§ˆë¬¸ì„ ì…ë ¥í•´ë³´ì„¸ìš”!
í˜„ìˆ˜ì— ëŒ€í•´ ë¬¼ì–´ë´ë„ ëª°ë¼ìš”
í•œêµ­ì–´ê°€ ì„œíˆ´ëŸ¬ì„œ ì˜¤íƒ€ê°€ ë§ìŠµë‹ˆë‹¤.
""")
st.caption("ê¹€í˜„ìˆ˜ ë¬¼ì–´ë´ë„ ëª¨ë¦„ / í•œêµ­ì–´ ì„œíˆ¼")

# ì„¸ì…˜ ìƒíƒœë¡œ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
if "messages" not in st.session_state:
    st.session_state.messages = []

# ëŒ€í™” ê¸°ë¡ ì¶œë ¥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

if user_input:
    # ìœ ì € ë©”ì‹œì§€ í™”ë©´ì— í‘œì‹œ
    st.chat_message("user").markdown(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})

    # ë¹ˆ ì±— ë©”ì‹œì§€ë¥¼ ë¯¸ë¦¬ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ìš©)
    assistant_message = st.chat_message("assistant")
    assistant_response = assistant_message.empty()

    full_reply = ""

    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë°›ê¸°
    response = client.chat.completions.create(
        #model = "gpt-4o",
        #model = "llama3-70b-8192", # Groq ëª¨ë¸
        model = "gemma2-9b-it",
        messages=st.session_state.messages,
        stream=True # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
    )

    for chunk in response:
        if chunk.choices[0].delta.content:
            full_reply += chunk.choices[0].delta.content
            assistant_response.markdown(full_reply + "â–Œ") # ì»¤ì„œ ëŠë‚Œ

    # ìŠ¤íŠ¸ë¦¬ë° ëë‚œ í›„ ìµœì¢… ë©”ì‹œì§€ í‘œì‹œ
    assistant_response.markdown(full_reply)
    st.session_state.messages.append({"role": "assistant", "content": full_reply})
