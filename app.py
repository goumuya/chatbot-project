import os
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI
from streamlit_webrtc import webrtc_streamer, AudioProcessorBase
import numpy as np
import tempfile
import soundfile as sf
import av

# .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
load_dotenv()

client = OpenAI(
    api_key=st.secrets["OPENAI_PAID_API_KEY"]
)

# ë§ˆì´í¬ ì²˜ë¦¬ í´ë˜ìŠ¤
class AudioProcessor(AudioProcessorBase):
    def __init__(self):
        self.recorded_frames = []
        self.volume = 0

    def recv_queued(self, frames):
        for frame in frames:
            audio = frame.to_ndarray().flatten().astype(np.int16)
            self.recorded_frames.append(audio)
            self.volume = int(np.linalg.norm(audio) / len(audio) * 10)
        return frames[-1]

# Streamlit ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="ğŸ§  ë§ë™ë¬´ ì±—ë´‡ ì—°ìŠµìš©", page_icon="ğŸ’¬")
st.title("ğŸ§  ì‹¬ì‹¬í’€ì´ ë§ë™ë¬´ ì±—ë´‡")
st.markdown("""
# ğŸ‘‹ í™˜ì˜í•©ë‹ˆë‹¤!
ì´ê³³ì€ ë§ë™ë¬´ ì±—ë´‡ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. 
ì„±ê²©ì„ ê³ ë¥´ê³ , ì•„ë˜ ì…ë ¥ì°½ì—ì„œ ê°„ë‹¨í•œ ìˆ˜ë‹¤ë¥¼ ì¦ê²¨ë³´ì„¸ìš”!
""")
st.caption("ê¹€í˜„ìˆ˜ ë¬¼ì–´ë´ë„ ëª¨ë¦…ë‹ˆë‹¤.ã… ")

# â–¶ ì„±ê²© í”„ë¡¬í”„íŠ¸ ì‚¬ì „
system_prompts = {
    "Sweetie": "You are an AI who always talks to people in a kind and gentle way. and You must answer in Korean.",
    "Lover" : "You are an AI who always communicates with people in a sweet and loving way. Please answer in a comfortable but loving way as much as you can to your lover. and You must answer in Korean",
    "Strictly": "You're an AI that only responds to you with a firm, strict tone. and You must answer in Korean.",
    "twisted": "You're an AI who only talks in a grumpy, grumpy tone. You reply in a slightly upset tone. Instead, and You must answer in Korean.",
    "Ignore": "You're an AI who talks in a chilly, hateful way for some reason. You respond in a slightly angry way. and You must answer in Korean."
}

# ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "personality" not in st.session_state:
    st.session_state.personality = "Sweetie"
if "mic_on" not in st.session_state:
    st.session_state.mic_on = False

# ë“œë¡­ë‹¤ìš´ìœ¼ë¡œ ì„±ê²© ì„ íƒ ê°€ëŠ¥í•˜ê²Œ
personality_list = list(system_prompts.keys())

with st.sidebar:
    st.markdown("## ğŸ¤– AI ì„±ê²© ì„¤ì •")
    selected = st.selectbox("í˜„ì¬ ì„±ê²©ì„ ì„ íƒí•˜ì„¸ìš”", personality_list,
                            index=personality_list.index(st.session_state.personality))
    if st.button("ğŸ’¬ ëŒ€í™” ë¦¬ì…‹"):
        st.session_state.messages = []
        st.rerun()

# ì„±ê²© ë³€ê²½ ê°ì§€
if selected != st.session_state.personality:
    st.session_state.personality = selected
    st.session_state.messages.append({
        "role": "assistant",
        "content": f"ì„±ê²©ì´ {selected}ìœ¼ë¡œ ë°”ë€Œì—ˆìŠµë‹ˆë‹¤."
    })

# ëŒ€í™” ê¸°ë¡ ì¶œë ¥
for message in st.session_state.messages:
    if "role" in message and "content" in message:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# ğŸ¤ ë§ˆì´í¬ on/off í† ê¸€ ë° í‘œì‹œ
st.markdown("## ğŸ¤ ë§ˆì´í¬ ì…ë ¥")
st.session_state.mic_on = st.toggle("ğŸ™ ë§ˆì´í¬ ì¼œê¸° / ë„ê¸°", value=st.session_state.mic_on)

user_input = None

if st.session_state.mic_on:
    st.success("ğŸ”´ ë§ˆì´í¬ê°€ ì¼œì¡ŒìŠµë‹ˆë‹¤. ë§ì„ ì‹œì‘í•˜ì„¸ìš”.")
    ctx = webrtc_streamer(
        key="speech",
        audio_processor_factory=AudioProcessor,
        media_stream_constraints={"audio": True, "video": False},
        async_processing=True,
    )

    if ctx.audio_processor:
        level = ctx.audio_processor.volume
        bar = "ğŸ”Š" * level + "â–«ï¸" * (10 - level)
        st.markdown(f"**ì†Œë¦¬ ì…ë ¥ ìƒíƒœ:** {bar}")

        if st.button("ğŸ“ ë§í•œ ë‚´ìš©ìœ¼ë¡œ ì§ˆë¬¸í•˜ê¸°"):
            if ctx.audio_processor.recorded_frames:
                audio_data = np.concatenate(ctx.audio_processor.recorded_frames)
                audio_duration = len(audio_data) / 48000

                if audio_duration < 0.1:
                    st.warning("âš ï¸ ìŒì„± ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ì¢€ ë” ê¸¸ê²Œ ë§í•´ë³´ì„¸ìš”.")
                else:
                    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmpfile:
                        sf.write(tmpfile.name, audio_data, 48000)

                        with open(tmpfile.name, "rb") as f:
                            transcript = client.audio.transcriptions.create(
                                model="whisper-1",
                                file=f,
                                language="ko"
                            )
                            user_input = transcript.text
                            st.chat_message("user").markdown(user_input)
                            st.session_state.messages.append({"role": "user", "content": user_input})
            else:
                st.warning("âš ï¸ ì•„ì§ ìŒì„±ì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë§ì„ í•˜ê³  ë‚˜ì„œ ë‹¤ì‹œ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
else:
    ctx = None

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
text_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
if text_input:
    user_input = text_input
    st.chat_message("user").markdown(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})

# GPT ì‘ë‹µ ìƒì„±
if user_input:
    assistant_message = st.chat_message("assistant")
    assistant_response = assistant_message.empty()
    full_reply = ""

    valid_messages = [
        msg for msg in st.session_state.messages
        if isinstance(msg, dict) and "role" in msg and "content" in msg
    ]

    response = client.chat.completions.create(
        model = "gpt-4o",
        messages=[{"role": "system", "content": system_prompts[st.session_state.personality]}] + valid_messages,
        stream=True
    )

    for chunk in response:
        if chunk.choices[0].delta.content:
            full_reply += chunk.choices[0].delta.content
            assistant_response.markdown(full_reply + "â–Œ")

    assistant_response.markdown(full_reply)
    st.session_state.messages.append({"role": "assistant", "content": full_reply})