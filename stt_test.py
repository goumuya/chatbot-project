import streamlit as st
from streamlit_webrtc import webrtc_streamer, AudioProcessorBase
from openai import OpenAI
import av, numpy as np, tempfile, soundfile as sf
import queue

client = OpenAI(api_key=st.secrets["OPENAI_PAID_API_KEY"])

# ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ í´ë˜ìŠ¤
class AudioProcessor(AudioProcessorBase):
    def __init__(self):
        self.audio_queue = queue.Queue()
        self.volume = 0
        self.recording = False  # ë‚´ë¶€ ìƒíƒœ ì§ì ‘ ìœ ì§€

    def recv_queued(self, frames):
        if self.recording:
            for frame in frames:
                audio = frame.to_ndarray().flatten().astype(np.int16)
                self.audio_queue.put(audio)
                max_vol = np.max(np.abs(audio))
                self.volume = int(np.linalg.norm(audio) / len(audio) * 10)
                print(f"ğŸ§ í”„ë ˆì„ ìˆ˜ì‹ ë¨ - ê¸¸ì´: {len(audio)}, ìµœëŒ€ ë³¼ë¥¨: {max_vol}")
        return frames[-1]

# Streamlit UI ì‹œì‘
st.title("ğŸ™ï¸ ì‹¤ì‹œê°„ ìŒì„± ì…ë ¥ â†’ í…ìŠ¤íŠ¸ ë³€í™˜ (ìµœì¢…ë²„ì „)")

# WebRTC ì‹œì‘
ctx = webrtc_streamer(
    key="speech-final",
    audio_processor_factory=AudioProcessor,
    media_stream_constraints={"audio": True, "video": False},
    async_processing=True,
)

# ë²„íŠ¼ UI
if ctx.audio_processor is not None and ctx.state.playing:
    col1, col2 = st.columns(2)

    with col1:
        if st.button("ğŸŸ¢ ë…¹ìŒ ì‹œì‘"):
            ctx.audio_processor.audio_queue = queue.Queue()  # í ì´ˆê¸°í™”
            ctx.audio_processor.recording = True
            st.success("ë…¹ìŒ ì‹œì‘ë¨")

    with col2:
        if st.button("â¹ ë…¹ìŒ ì¢…ë£Œ ë° ë³€í™˜"):
            ctx.audio_processor.recording = False
            frames = []

            q = ctx.audio_processor.audio_queue
            while not q.empty():
                frames.append(q.get())

            st.write(f"ğŸ§ª í”„ë ˆì„ ìˆ˜: {len(frames)}")

            if frames:
                audio_data = np.concatenate(frames)
                audio_duration = len(audio_data) / 48000
                max_volume = np.max(np.abs(audio_data))

                st.markdown("### ğŸ“Š ì˜¤ë””ì˜¤ ìˆ˜ì¹˜ ì •ë³´")
                st.write({
                    "ê¸¸ì´": len(audio_data),
                    "ìƒ˜í”Œë§ ë ˆì´íŠ¸": 48000,
                    "ì´ ì§€ì†ì‹œê°„ (ì´ˆ)": round(audio_duration, 3),
                    "ìµœëŒ€ê°’": int(np.max(audio_data)),
                    "ìµœì†Œê°’": int(np.min(audio_data)),
                    "í‰ê· ê°’": round(float(np.mean(audio_data)), 2),
                    "í‘œì¤€í¸ì°¨": round(float(np.std(audio_data)), 2)
                })

                if max_volume < 300:
                    st.warning("âš ï¸ ê°ì§€ëœ ìŒì„±ì´ ì—†ìŠµë‹ˆë‹¤.")
                elif audio_duration < 0.5:
                    st.warning("âš ï¸ ìŒì„± ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
                else:
                    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmpfile:
                        sf.write(tmpfile.name, audio_data, 48000)
                        with open(tmpfile.name, "rb") as f:
                            try:
                                transcript = client.audio.transcriptions.create(
                                    model="whisper-1",
                                    file=f,
                                    language="ko"
                                )
                                st.success("ğŸ—£ï¸ ê²°ê³¼:")
                                st.write(transcript.text)
                            except Exception as e:
                                st.error(f"âŒ ë³€í™˜ ì‹¤íŒ¨: {e}")
            else:
                st.warning("âš ï¸ í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë§ì„ í•´ë³´ì„¸ìš”.")

    # ì‹¤ì‹œê°„ ë³¼ë¥¨ ì‹œê°í™”
    level = ctx.audio_processor.volume
    bar = "ğŸ”Š" * level + "â–«ï¸" * (10 - level)
    st.markdown(f"**ì‹¤ì‹œê°„ ë³¼ë¥¨:** {bar} (ìˆ˜ì¹˜: {level})")
else:
    st.info("ğŸ™ï¸ ë§ˆì´í¬ ì—°ê²° ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤...")
