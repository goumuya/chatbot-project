import streamlit as st
import sqlite3
from collections import Counter, defaultdict
import pandas as pd
import numpy as np
import json
from google.cloud import language_v1
from google.oauth2 import service_account
import os

st.set_page_config(page_title="ğŸ“Š ëŒ€í™” ë¡œê·¸ ë¶„ì„", page_icon="ğŸ“Š")
st.title("ğŸ“Š ëŒ€í™” ë¡œê·¸ ë¶„ì„ ë„êµ¬")

DB_FILE = os.path.join(os.getcwd(), "chat_log.db")
# ì„œë¹„ìŠ¤ ê³„ì • í‚¤ ê°€ì ¸ì˜¤ê¸°
credentials_info = st.secrets["google_service_account"]
credentials = service_account.Credentials.from_service_account_info(credentials_info)
client = language_v1.LanguageServiceClient(credentials=credentials)

# DB ì¡´ì¬ í™•ì¸
try:
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()
except Exception as e:
    st.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
    st.stop()


# ì¼ë°˜ëŒ€í™” í…Œì´ë¸” ì¡´ì¬ í™•ì¸
cursor.execute("""
    SELECT name FROM sqlite_master WHERE type='table' AND name='normal_logs'
""")
if not cursor.fetchone():
    st.warning("â— 'normal_logs' í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

# ë°ì´í„° ë¡œë”©
df = pd.read_sql_query("SELECT * FROM normal_logs", conn)

if df.empty:
    st.warning("â— ë¶„ì„í•  ëŒ€í™” ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

# êµ¬ê¸€ ê°ì •ë¶„ì„ API ì‚¬ìš©
emotion_scores = defaultdict(list)

for _, row in df.iterrows():
    text = row["assistant"]
    character = row["character"]
    if not isinstance(text, str) or not text.strip():
        continue
    document = language_v1.Document(
        content=text,
        type_=language_v1.Document.Type.PLAIN_TEXT,
        language="ko"
    )
    try:
        response = client.analyze_sentiment(request={"document": document})
        sentiment = response.document_sentiment
        emotion_scores[character].append({
            "score": sentiment.score,
            "magnitude": sentiment.magnitude
        })
    except Exception as e:
        continue


# ê¸°ë³¸ í†µê³„
st.subheader("ğŸ—£ï¸ ì¼ë°˜ ëŒ€í™” ë¡œê·¸ ë¶„ì„")
st.markdown(f"- ğŸ‘¤ ì‚¬ìš©ì ë©”ì‹œì§€ ìˆ˜: **{len(df)}**")
st.markdown(f"- ğŸ¤– ì±—ë´‡ ì‘ë‹µ ìˆ˜: **{len(df)}**")

# 1. ì„±ê²©ë³„ í‰ê·  ë‹µë³€ ê¸¸ì´
st.subheader("1ï¸âƒ£ ì„±ê²©ë³„ í‰ê·  ë‹µë³€ ê¸¸ì´")
df["assistant_length"] = df["assistant"].str.len()
avg_length = df.groupby("character")["assistant_length"].mean().sort_values(ascending=False)
st.bar_chart(avg_length)

# 2. ì„±ê²©ë³„ ìì£¼ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ Top 10
st.subheader("2ï¸âƒ£ ì„±ê²©ë³„ ìì£¼ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ Top 10")
for char in df["character"].unique():
    st.markdown(f"**ğŸ§  {char}**")
    words = " ".join(df[df["character"] == char]["assistant"]).split()
    word_counts = Counter(words)
    top_words = pd.DataFrame(word_counts.most_common(10), columns=["ë‹¨ì–´", "ë¹ˆë„ìˆ˜"])
    st.dataframe(top_words)

# 3. ê°ì • ì ìˆ˜ ë¶„ì„ (êµ¬ê¸€ í´ë¼ìš°ë“œ API ê¸°ë°˜)
st.subheader("3âƒ£ï¸ ì„±ê²©ë³„ ê°ì • ì ìˆ˜ í‰ê·  (Google Cloud NLP)")
result_data = []
for char, scores in emotion_scores.items():
    if not scores:
        continue
    avg_score = sum(s["score"] for s in scores) / len(scores)
    avg_magnitude = sum(s["magnitude"] for s in scores) / len(scores)
    result_data.append({
        "ì„±ê²©" : char,
        "í‰ê·  ê°ì • ì ìˆ˜" : round(avg_score, 3),
        "ê°ì • ê°•ë„" : round(avg_magnitude, 3),
        "ëŒ€í™” ìˆ˜" : len(scores) 
    })

result_df = pd.DataFrame(result_data).sort_values(by="í‰ê·  ê°ì • ì ìˆ˜", ascending=False)
st.dataframe(result_df.set_index("ì„±ê²©"))
st.bar_chart(result_df.set_index("ì„±ê²©")[["í‰ê·  ê°ì • ì ìˆ˜"]])

# 4. ì‹œê°„ëŒ€ ë¶„í¬
st.subheader("4ï¸âƒ£ ì„±ê²©ë³„ ì‘ë‹µ ì‹œê°„ëŒ€ ë¶„í¬")
df["created_at"] = pd.to_datetime(df["created_at"])
df["hour"] = df["created_at"].dt.hour
hourly_dist = df.groupby(["character", "hour"]).size().unstack(fill_value=0)
st.line_chart(hourly_dist.T)

# 5. ì‚¬ìš©ì ë©”ì‹œì§€ ê¸¸ì´ì™€ ì‘ë‹µ ê¸¸ì´ ìƒê´€ê´€ê³„
st.subheader("5ï¸âƒ£ ì‚¬ìš©ì ë©”ì‹œì§€ ê¸¸ì´ì™€ ì‘ë‹µ ê¸¸ì´ ìƒê´€ê´€ê³„")
df["user_length"] = df["user"].str.len()
st.scatter_chart(df[["user_length", "assistant_length"]])

# 6. ê°íƒ„ì‚¬ ì‚¬ìš© ë¹ˆë„ ë¶„ì„
st.subheader("6ï¸âƒ£ ì„±ê²©ë³„ ê°íƒ„ì‚¬ ì‚¬ìš© ë¹ˆë„")
exclamations = ["í—‰", "ì™€", "ìš°ì™€", "ëŒ€ë°•", "ì–´ë¨¸", "ì˜¤!", "ìš°ì•—", "êº„", "í—"]
def count_exclamations(text):
    return sum(text.count(e) for e in exclamations)
df["exclam_count"] = df["assistant"].apply(count_exclamations)
exc_counts = df.groupby("character")["exclam_count"].sum().sort_values(ascending=False)
st.bar_chart(exc_counts)

# ì¶”ê°€: ê¸°ì¡´ì— ìˆë˜ ì‚¬ìš©ì ë©”ì‹œì§€ Top 10 ë‹¨ì–´ë„ ê°™ì´ ì¶œë ¥
st.subheader("ğŸ“Œ ì‚¬ìš©ì ë©”ì‹œì§€ Top 10 ë‹¨ì–´")
user_text = " ".join(df["user"].tolist())
word_counts = Counter(user_text.split())
top_words = word_counts.most_common(10)
st.dataframe(pd.DataFrame(top_words, columns=["ë‹¨ì–´", "ë¹ˆë„ìˆ˜"]))

# ì„±ê²©ë³„ ëŒ€í™” ìˆ˜
st.subheader("ğŸ§  ì„±ê²©ë³„ ëŒ€í™” ìˆ˜")
char_count = df["character"].value_counts().rename_axis("ì„±ê²©").reset_index(name="ëŒ€í™” ìˆ˜")
st.bar_chart(char_count.set_index("ì„±ê²©"))

conn.close()